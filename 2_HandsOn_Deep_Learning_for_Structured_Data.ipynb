{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34c275b08da8be8",
   "metadata": {},
   "source": [
    "# Exercise: Deep Learning for Structured Data\n",
    "\n",
    "In this exercise we will implement a neural network more or less from scratch using numpy for matrix operations. Clearly, we would never be doing this in practice but rather use state-of-the-art software libraries.\n",
    "Still, implementing a neural network with its training routine by hand deepens our understanding and demystifies the technology. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e0240-b756-44f8-a7f6-51809a44711b",
   "metadata": {},
   "source": [
    "#### Importing libraries and tools"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7b9cbe8-f5b6-4715-b67e-a48224b07e83",
   "metadata": {
    "scrolled": true
   },
   "source": "!pip install numpy medmnist pandas scipy matplotlib",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ccbbc3a280cabd8",
   "metadata": {},
   "source": [
    "import abc\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fb6ebda9-730c-4adb-abcd-dd7229fdbc53",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f60228e-beae-4dcb-8aae-5404e7fa7d8f",
   "metadata": {},
   "source": [
    "def plot_training_curve(loss: list, val_loss: list, fill_between=True):\n",
    "    epochs_range = range(len(val_loss))\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(epochs_range, loss, label=\"training loss\")\n",
    "    plt.plot(epochs_range, val_loss, label=\"validation loss\")\n",
    "    if fill_between:\n",
    "        plt.fill_between(epochs_range, loss, val_loss, color=\"lightpink\", alpha=0.4, hatch=\"-\", label=\"difference\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(\"training and validation loss\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b861b5c1-d40a-4b3c-99ea-85ad90ea75ec",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4c720d50c88a1",
   "metadata": {},
   "source": [
    "We use the scikit-learn inbuilt breast cancer dataset. It's features are computed from digitized images of fine needle aspirates of a breast mass.\n",
    "This is structured data, since it is available in a standardized tabular format."
   ]
  },
  {
   "cell_type": "code",
   "id": "36f0d58e3266571",
   "metadata": {},
   "source": [
    "df = load_breast_cancer(as_frame=True)[\"frame\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "475d5e0fd521ff37",
   "metadata": {},
   "source": [
    "The data is obtained from images such as the one below. For example, the mean radius is the average of the radi of the extracted nuclei visible in the picture.\n",
    "\n",
    "![Image of Yaktocat](https://www.researchgate.net/profile/Nick-Street/publication/2512520/figure/fig2/AS:279373199495169@1443619169198/Snakes-After-Convergence-to-Cell-Nucleus-Boundaries-These-contours-are-the-nal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832db03135ff2891",
   "metadata": {},
   "source": [
    "Note that every good machine learning engineer should at this point check for potential issues in the data, and preprocess it as appropriate.\n",
    "In this guided exercise we will completely ignore this phase; please **never** do this with real data.\n",
    "\n",
    "For our simplified classification task, we will select the malignancy (means the cancer is not harmless) as target (already called target in our data, this is our `y`).\n",
    "We will use all other columns as feature variables (our `X`)."
   ]
  },
  {
   "cell_type": "code",
   "id": "4d88154d-3bff-4739-81df-6ed9871b0c0a",
   "metadata": {},
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = df[list(df.columns)[:-1]].values # only first 2 columns\n",
    "y = df[[\"target\"]].values.ravel()\n",
    "X.shape, y.shape"
   ],
   "id": "5380b309d0efdcf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's look at the distribution of the classes in the training dataset to see if we have a balanced dataset",
   "id": "6d5e98613b46f593"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "labels = {0: \"Benign\", 1: \"Malignant\"}",
   "id": "2883761a-fe79-45e9-9bcb-a523faf76778",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def count_labels(labels, classnames) -> dict:\n",
    "    classnames = list(classnames)\n",
    "    dictionary = dict.fromkeys(classnames, 0)\n",
    "    for label in labels:\n",
    "        dictionary[classnames[label]] += 1\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "class_counts = count_labels(y.ravel(), labels.values())\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.xticks(rotation=45)\n",
    "_ = plt.title(\"Distribution of classes in the whole dataset\")"
   ],
   "id": "d8fcf6c041207f7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our goal is to create a model to predict whether an image presents malignant breast cancer.\n",
    "However, we would like this model to work not only for the images in this dataset, but also for new ones captured in the future.\n",
    "In other words, the model should _generalize_ well to new data.\n",
    "\n",
    "In order to check how good our model is in predicting malignancy of yet unseen images, we put 30% of the data to the side and split this into validation and test set. The test set must only be used once for the final validation, while the validation set can be used for hyperparameter tuning.\n",
    "\n",
    "The model will be trained only on the first 70% of the data, our _training set_. Since we have an imbalance in the data, we will use the stratify parameter to ensure that the class distribution in the splits is the same as in the original dataset."
   ],
   "id": "e430712f35d95729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True, stratify=y)\n",
    "x_train = x_train\n",
    "x_val, y_val = x_test[:len(x_test) // 2], y_test[:len(y_test) // 2]\n",
    "x_test, y_test = x_test[len(x_test) // 2:], y_test[len(y_test) // 2:]\n",
    "y_train, y_val, y_test = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
    "assert x_test.shape != x_val.shape or x_test != x_val\n",
    "assert y_test.shape != y_val.shape or y_test != y_val"
   ],
   "id": "26e706036b4f452a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The features have very different scales, so we standardize first.",
   "id": "22e665d36e55c41e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train, x_val, x_test = scaler.transform(x_train).T, scaler.transform(x_val).T, scaler.transform(x_test).T"
   ],
   "id": "964194ad875de958",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90696e2a-c4e4-4546-b42a-118845ae4c63",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c20e1-1b0f-406c-bba2-ad117149c5ef",
   "metadata": {},
   "source": [
    "In this section, we define and implement the different components of a multi-level perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52588142-32b8-4e43-9169-324eb448aa28",
   "metadata": {},
   "source": [
    "### Activations & Losses"
   ]
  },
  {
   "cell_type": "code",
   "id": "edd24107-786b-4696-837a-a5d45dcf3055",
   "metadata": {},
   "source": [
    "class AbstractActivationFunction(abc.ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for activation functions\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, z: np.ndarray):\n",
    "        \"\"\"\n",
    "        Computes the value of the activation function element wise\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def backward(self, z: np.ndarray):\n",
    "        \"\"\"\n",
    "        Computes the derivative of the activation function element wise\n",
    "        \"\"\"\n",
    "        ...\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a667f73ed697e5f",
   "metadata": {},
   "source": [
    "class ReLU(AbstractActivationFunction):\n",
    "    \"\"\"\n",
    "    Implements the ReLU activation function and its derivative\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    def backward(self, z: np.ndarray) -> np.ndarray:\n",
    "        d = np.ones_like(z)\n",
    "        d[z < 0] = 0\n",
    "        return d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b37cfd3d-d207-4f2f-9fa0-f5d5af5857cf",
   "metadata": {},
   "source": [
    "**Exercise 1: Implement the Sigmoid activation function and its derivative**"
   ]
  },
  {
   "cell_type": "code",
   "id": "0cac2f99-a9a3-4edb-81b1-0a8877e6a8b4",
   "metadata": {},
   "source": [
    "class Sigmoid(AbstractActivationFunction):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation function and its derivative\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, z: np.ndarray):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid function element wise\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###\n",
    "        ...\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        ### START YOUR CODE ###\n",
    "        ...\n",
    "        ### END YOUR CODE ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "128523fe-850a-42cb-998b-b4c951386f1e",
   "metadata": {},
   "source": [
    "**Click below for the solution**"
   ]
  },
  {
   "cell_type": "code",
   "id": "6c17c5d3bc5b1e53",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#@title Solution for Sigmoid activation function\n",
    "class Sigmoid(AbstractActivationFunction):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation function and its derivative\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, z: np.ndarray):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid function element wise\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        s = self.forward(z)\n",
    "        return s * (1 - s)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94e5955d-b6da-419e-aa83-4423287a2808",
   "metadata": {},
   "source": [
    "class NormInitializer:\n",
    "\n",
    "    def initialize_weights(self, size: int, mu: float = 0.0, sigma: float = 1.0) -> np.ndarray:\n",
    "        return np.random.normal(size=size, loc=mu, scale=sigma)\n",
    "\n",
    "    def initialize_bias(self, size: int, mu: float = 0.0, sigma: float = 1.0) -> np.ndarray:\n",
    "        return np.random.normal(size=size, loc=mu, scale=sigma)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a358915-57b7-47a5-9d3d-9642dbdb41c0",
   "metadata": {},
   "source": [
    "class AbstractLoss(abc.ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for loss functions\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, y: np.ndarray, prob: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the loss\n",
    "        :param y: true labels, shape (n_classes, batch_size)\n",
    "        :param prob: predicted probabilities, shape (n_classes, batch_size)\n",
    "        :param class_weights: class weights for weighting loss according to class balance, shape (n_classes,)\n",
    "        :return: the loss value as a float\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def backward(self, y: np.ndarray, prob: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass of the loss, computes the gradient of the loss with respect to the logits\n",
    "        :param y: true labels, shape (n_classes, batch_size)\n",
    "        :param prob: predicted probabilities, shape (n_classes, batch_size)\n",
    "        :param class_weights: class weights for weighting loss according to class balance, shape (n_classes,)\n",
    "        :return: gradient of the loss with respect to the logits, shape (n_classes, batch_size)\n",
    "        \"\"\"\n",
    "        ..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6026cdb2-24e1-4059-8318-94cd74ee1a03",
   "metadata": {},
   "source": [
    "class BinaryCrossEntropy(AbstractLoss):\n",
    "    \"\"\"\n",
    "    Implements the binary cross entropy loss function and its derivative\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, y: np.ndarray, prob: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes the binary cross entropy loss\n",
    "        :param y: true labels, shape (n_classes, batch_size)\n",
    "        :param prob: predicted probabilities, shape (n_classes, batch_size)\n",
    "        :param class_weights: class weights for weighting loss according to class balance, shape (n_classes,)\n",
    "        :return: the binary cross entropy loss value as a float\n",
    "        \"\"\"\n",
    "        assert np.max(y) <= 1, y\n",
    "        assert np.max(prob) <= 1, prob\n",
    "        prob = np.clip(prob, 1e-8, 1 - 1e-8)\n",
    "        loss = (y * np.log(prob) + (1 - y) * np.log(1 - prob))\n",
    "        return -np.mean(np.average(loss, axis=-1))\n",
    "\n",
    "    def backward(self, y: np.ndarray, prob: np.ndarray, class_weights: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass of the binary cross entropy loss, computes the gradient of the loss with respect to the logits\n",
    "        :param y: true labels, shape (n_classes, batch_size)\n",
    "        :param prob: predicted probabilities, shape (n_classes, batch_size)\n",
    "        :param class_weights: class weights for weighting loss according to class balance, shape (n_classes,)\n",
    "        :return: gradient of the loss with respect to the logits, shape (n_classes, batch_size)\n",
    "        \"\"\"\n",
    "        return -(y / (prob + 1e-8)) + ((1 - y) / (1 - prob + 1e-8))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f2c8e53-81cb-4c55-a7df-d2135dc163fb",
   "metadata": {},
   "source": [
    "#### Layers"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe6e5f7a-1e9e-4584-ac85-1d82d11ac77f",
   "metadata": {},
   "source": [
    "class AbstractLayer(abc.ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 layer_id: str,\n",
    "                 n_neurons: int,\n",
    "                 n_neurons_prev: int,\n",
    "                 activation: AbstractActivationFunction,\n",
    "                 initializer: NormInitializer) -> None:\n",
    "        \"\"\"\n",
    "        Abstract base class for layers in an MLP\n",
    "        :param layer_id: unique identifier for the layer\n",
    "        :param n_neurons: number of neurons in the layer\n",
    "        :param n_neurons_prev: number of neurons in the previous layer\n",
    "        :param activation: activation function for the layer\n",
    "        :param initializer: initializer for the weights and biases of the layer\n",
    "        \"\"\"\n",
    "        self.layer_id = layer_id\n",
    "        self.n_neurons = n_neurons\n",
    "        self.n_neurons_prev = n_neurons_prev\n",
    "        self.initializer = initializer\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weights = None  # shape (n_neurons, n_neurons_prev)\n",
    "        self.bias = None  # shape (n_neurons, 1)\n",
    "        self.logits = None  # shape (n_neurons, batch_size)\n",
    "        self.activations = None  # shape(n_neurons, batch_size)\n",
    "        self.grad_logits = None  # shape(n_neurons, batch_size)\n",
    "\n",
    "    def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases of the layer\n",
    "        \"\"\"\n",
    "        sigma_w = np.sqrt(2.0 / (self.n_neurons + self.n_neurons_prev))  # suited for sigmoid activation function\n",
    "        sigma_b = np.sqrt(1.0 / self.n_neurons)\n",
    "        self.weights = self.initializer.initialize_weights(size=(self.n_neurons, self.n_neurons_prev), mu=0.0,\n",
    "                                                           sigma=sigma_w)\n",
    "        self.bias = self.initializer.initialize_bias(size=(self.n_neurons, 1), mu=0.0, sigma=sigma_b)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, activations_prev: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass of the layer, computes the logits and activations of the layer\n",
    "        :param activations_prev: activations of the previous layer, shape (n_neurons_prev, batch_size)\n",
    "        :return: activations of the layer, shape (n_neurons, batch_size)\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def backward(self, grad_activations: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass of the layer, computes the gradient with respect to the activations of the previous layer\n",
    "        :param grad_activations: gradient with respect to the activations of the current layer, shape (n_neurons, batch_size)\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    def gradient_weights(self, activations_prev: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient of the loss with respect to the weights of the layer\n",
    "        :param activations_prev: activations of the previous layer, shape (n_neurons_prev, batch_size)\n",
    "        :return: gradient of the loss with respect to the weights, shape (n_neurons, n_neurons_prev)\n",
    "        \"\"\"\n",
    "        grad_weights = self.grad_logits.dot(activations_prev.T)\n",
    "        return grad_weights\n",
    "\n",
    "    def gradient_bias(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient of the loss with respect to the bias of the layer\n",
    "        :return: gradient of the loss with respect to the bias, shape (n_neurons, 1)\n",
    "        \"\"\"\n",
    "        grad_bias = self.grad_logits.sum(axis=-1, keepdims=True)\n",
    "        return grad_bias"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f089370-2b5c-40e4-9c81-ae9b1a78bc31",
   "metadata": {},
   "source": [
    "**Exercise 2: Forward and backward propagation of layers**"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed65e7fe-fdfd-4a2c-a627-3449fed6804a",
   "metadata": {},
   "source": [
    "class Layer(AbstractLayer):\n",
    "\n",
    "    def forward(self, activations_prev: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass of the layer, computes the logits and activations of the layer\n",
    "        :param activations_prev: activations of the previous layer, shape (n_neurons_prev, batch_size)\n",
    "        :return: activations of the layer, shape (n_neurons, batch_size)\n",
    "        \"\"\"\n",
    "        np.testing.assert_equal(activations_prev.shape[0], self.n_neurons_prev)\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        ...\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def backward(self, grad_activations: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass of the layer, computes the gradient with respect to the activations of the previous layer\n",
    "        :param grad_activations: gradient with respect to the activations of the current layer, shape (n_neurons, batch_size)\n",
    "        \"\"\"\n",
    "        n_samples = self.logits.shape[1]\n",
    "        np.testing.assert_equal(grad_activations.shape, (self.n_neurons, n_samples))\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        ...\n",
    "        ### END YOUR CODE ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5cf20971-b64a-4800-b884-c109a108e2bc",
   "metadata": {},
   "source": [
    "**Click below for the solution**"
   ]
  },
  {
   "cell_type": "code",
   "id": "1aff0de9-3365-4d04-88d8-6f2f4fb2e565",
   "metadata": {},
   "source": [
    "#@title Solution for Layer forward and backward propagation\n",
    "class Layer(AbstractLayer):\n",
    "\n",
    "    def forward(self, activations_prev: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass of the layer, computes the logits and activations of the layer\n",
    "        :param activations_prev: activations of the previous layer, shape (n_neurons_prev, batch_size)\n",
    "        :return: activations of the layer, shape (n_neurons, batch_size)\n",
    "        \"\"\"\n",
    "        np.testing.assert_equal(activations_prev.shape[0], self.n_neurons_prev)\n",
    "\n",
    "        ### START YOUR CODE ###\n",
    "        self.logits = np.dot(self.weights, activations_prev) + self.bias\n",
    "        self.activations = self.activation.forward(self.logits)\n",
    "\n",
    "        return self.activations\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def backward(self, grad_activations: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass of the layer, computes the gradient with respect to the activations of the previous layer\n",
    "        :param grad_activations: gradient with respect to the activations of the current layer, shape (n_neurons, batch_size)\n",
    "        \"\"\"\n",
    "        nsamples = self.logits.shape[1]\n",
    "        np.testing.assert_equal(grad_activations.shape, (self.n_neurons, nsamples))\n",
    "\n",
    "        # Compute gradient with respect to logits\n",
    "        self.grad_logits = grad_activations * self.activation.backward(self.logits)\n",
    "        np.testing.assert_equal(self.grad_logits.shape, (self.n_neurons, nsamples))\n",
    "\n",
    "        # Compute gradient with respect to previous layer activations\n",
    "        grad_activations_prev = self.weights.T.dot(self.grad_logits)\n",
    "        np.testing.assert_equal(grad_activations_prev.shape, (self.n_neurons_prev, nsamples))\n",
    "\n",
    "        return grad_activations_prev"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b74e244-ed24-4db1-8164-54854689a082",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "id": "9efe0dc9-d866-4822-8529-31e7899a8fac",
   "metadata": {},
   "source": [
    "class AbstractMultiLayerPerceptron(abc.ABC):\n",
    "\n",
    "    def __init__(self, \n",
    "                 neurons_per_layer: list[int], \n",
    "                 activation: AbstractActivationFunction, \n",
    "                 initializer: NormInitializer) -> None:\n",
    "        \"\"\"\n",
    "        Abstract base class for a multi layer perceptron\n",
    "        :param neurons_per_layer: list of integers, number of neurons per layer\n",
    "        :param activation: activation function for the hidden layers\n",
    "        :param initializer: initializer for the weights and biases of the layers\n",
    "        \"\"\"\n",
    "        self.layers = []  # list of layers (instances of class Layer or Softmax or the like); ordered along the forward path.\n",
    "        self.number_layers = -1  # number of layers (excluding input layer, excluding softmax layer)\n",
    "        self.x = None  # input data for one iteration (forward and backward)\n",
    "\n",
    "        self.number_layers = len(neurons_per_layer)\n",
    "\n",
    "        for layer_id in range(1, self.number_layers):\n",
    "            if layer_id != self.number_layers - 1:\n",
    "                layer_act = activation\n",
    "            else:\n",
    "                layer_act = Sigmoid()\n",
    "            n_neurons = neurons_per_layer[layer_id]\n",
    "            n_neurons_prev = neurons_per_layer[layer_id - 1]\n",
    "            self.layers.append(\n",
    "                Layer(\n",
    "                    layer_id, n_neurons, n_neurons_prev,\n",
    "                    layer_act, initializer))\n",
    "\n",
    "    def initialize(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases of the layers\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.initialize()\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass of the MLP\n",
    "        :param x: input data, shape (n_features, batch_size)\n",
    "        :return: output of the MLP, shape (n_classes, batch_size)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        a = self.x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            a = layer.forward(a)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward(self, grady: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass of the MLP to compute the gradients\n",
    "        :param grady: gradient of the loss with respect to the output of the MLP, shape (n_classes, batch_size)\n",
    "        :return: gradient of the loss with respect to the input of the MLP, shape (n_features, batch_size)\n",
    "        \"\"\"\n",
    "        grad = grady\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def step_params(self, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Updates the parameters of the MLP\n",
    "        :param learning_rate: learning rate for the parameter update\n",
    "        \"\"\"\n",
    "        ...\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29aa9266-eabb-40aa-bde7-edf766904aac",
   "metadata": {},
   "source": [
    "**Exercise 3: Implement the parameter update step for the MLP**"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e5bf31e-45ce-4b30-9d00-318ea6d30159",
   "metadata": {},
   "source": [
    "class MultiLayerPerceptron(AbstractMultiLayerPerceptron):\n",
    "    def step_params(self, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Updates the parameters of the MLP\n",
    "        :param learning_rate: learning rate for the parameter update\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ### \n",
    "        ...\n",
    "        ### END YOUR CODE ### "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fcd912ff-073f-4748-8e29-7348da8c325b",
   "metadata": {},
   "source": [
    "**Click below for the solution**"
   ]
  },
  {
   "cell_type": "code",
   "id": "794da1df-7ae1-48de-b54c-1b03c3cb8685",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#@title Solution for the parameter update step of the MLP\n",
    "class MultiLayerPerceptron(AbstractMultiLayerPerceptron):\n",
    "    def step_params(self, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Updates the parameters of the MLP\n",
    "        :param learning_rate: learning rate for the parameter update\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ### \n",
    "        prev_activations = self.x\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.weights -= learning_rate * layer.gradient_weights(prev_activations)\n",
    "            layer.bias -= learning_rate * layer.gradient_bias()\n",
    "\n",
    "            prev_activations = layer.activations\n",
    "        ### END YOUR CODE ### "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3de58e74-52cf-4d56-bf45-2947d0fb7ea9",
   "metadata": {},
   "source": [
    "### Forward Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe3b12-2dd6-4bdb-818c-419294731248",
   "metadata": {},
   "source": [
    "If your implementation is incorrect, this should throw an AssertionError."
   ]
  },
  {
   "cell_type": "code",
   "id": "024ebc42-8492-4eb5-ba03-28434ef670df",
   "metadata": {},
   "source": [
    "x = np.arange(9).reshape(3, 3).astype(float)\n",
    "layer_sizes = [3, 16, 16, 1]\n",
    "np.random.seed(42)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_sizes, Sigmoid(), NormInitializer())\n",
    "for layer_id in range(len(layer_sizes) - 1):\n",
    "    layer = mlp.layers[layer_id]\n",
    "    layer.weights = np.ones(shape=(layer.n_neurons, layer.n_neurons_prev), dtype='float') * 0.1\n",
    "    layer.bias = np.zeros(shape=(layer.n_neurons_prev, 1), dtype='float')\n",
    "\n",
    "mlp.initialize()\n",
    "y = mlp.forward(x)\n",
    "np.testing.assert_equal(y.shape, (1, 3))\n",
    "\n",
    "y_expected = np.array([[0.54265 , 0.541717, 0.540696]])\n",
    "np.testing.assert_array_almost_equal(y, y_expected, decimal=6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "88ff1e89-4e9f-457a-9197-65f029d4a0ef",
   "metadata": {},
   "source": [
    "### Gradient Checking\n",
    "\n",
    "Of course plotting the gradient gives us immediate insights whether our gradient makes sense, but it's hard to tell when the gradient is just slightly off."
   ]
  },
  {
   "cell_type": "code",
   "id": "f90865e775736849",
   "metadata": {},
   "source": [
    "bce = BinaryCrossEntropy()\n",
    "sigmoid = Sigmoid()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "p_values = np.linspace(0.00, 1, 100)\n",
    "values = np.linspace(-10, 10, 100)\n",
    "positive = np.array([[1]])\n",
    "negative = np.array([[0]])\n",
    "\n",
    "bce_y1 = [bce.forward(positive, np.array([[p]])) for p in p_values]\n",
    "bce_grad_1 = [np.mean(bce.backward(positive, np.array([[p]]))) for p in p_values]\n",
    "\n",
    "sig_y1 = [sigmoid.forward(v) for v in values]\n",
    "sig_grad_1 = [sigmoid.backward(v) for v in values]\n",
    "\n",
    "def show_gradient(x: np.ndarray, y, grad: np.ndarray, title: str, idx: int, y_lim=None) :\n",
    "    plt.subplot(1, 3, idx)\n",
    "    plt.plot(x, y, label=\"Y\", color='blue')\n",
    "    plt.plot(x, grad, label=\"Gradient\", color='red')\n",
    "    if y_lim is not None:\n",
    "        plt.ylim(y_lim)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "show_gradient(p_values, bce_y1, bce_grad_1, title=\"BCE Loss\", idx=1, y_lim=(-10, 10))\n",
    "show_gradient(values, sig_y1, sig_grad_1, title=\"Sigmoid\", idx=2)"
   ],
   "id": "d4bc104166f2f10d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "986d5c868735995",
   "metadata": {},
   "source": [
    " We can also check the gradient numerically with the finite differences method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d3a18776c1bd4",
   "metadata": {},
   "source": [
    "Checking the cross-entropy loss gradient"
   ]
  },
  {
   "cell_type": "code",
   "id": "6298d042-5174-454a-b8f5-7cedc5ebed35",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "epsilon = 1e-8\n",
    "tolerance = 1e-7\n",
    "\n",
    "y_pred = np.random.uniform(0.0, 1.0, (1, 16))\n",
    "y_true = (np.random.uniform(0.0, 1.0, (1, 16)) > 0.5).astype(float)\n",
    "\n",
    "loss = bce.forward(y_true, y_pred)\n",
    "grad_analytical = bce.backward(y_true, y_pred)\n",
    "\n",
    "predictions_plus = y_pred + epsilon\n",
    "predictions_minus = y_pred - epsilon\n",
    "\n",
    "loss_plus = bce.forward(y_true, predictions_plus)\n",
    "loss_minus = bce.forward(y_true, predictions_minus)\n",
    "\n",
    "grad_numerical = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "print(f\"Measured Error: {np.mean(grad_analytical) - grad_numerical}\")\n",
    "assert abs(np.mean(grad_analytical) - grad_numerical) < tolerance\n",
    "print(\"Gradient check passed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b47d259a908af8a3",
   "metadata": {},
   "source": [
    "Checking the Sigmoid gradient"
   ]
  },
  {
   "cell_type": "code",
   "id": "d272850690b992f7",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "epsilon = 1e-8\n",
    "tolerance = 1e-7\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "y_pred = np.random.uniform(-10, 10.0, (1, 16))\n",
    "\n",
    "loss = sigmoid.forward(y_pred)\n",
    "grad_analytical = sigmoid.backward(y_pred)\n",
    "\n",
    "predictions_plus = y_pred + epsilon\n",
    "predictions_minus = y_pred - epsilon\n",
    "\n",
    "loss_plus = sigmoid.forward(predictions_plus)\n",
    "loss_minus = sigmoid.forward(predictions_minus)\n",
    "\n",
    "grad_numerical = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "print(f\"Measured Error: {grad_analytical - grad_numerical}\")\n",
    "assert abs(np.mean(grad_analytical - grad_numerical)) < tolerance\n",
    "print(\"Gradient check passed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0e99103e-742c-4a63-9fc1-1239646ef00e",
   "metadata": {},
   "source": [
    "tolerance = 3e-7\n",
    "epsilon = 1e-8\n",
    "np.random.seed(41)\n",
    "\n",
    "layer_sizes = [16, 16, 1]\n",
    "mlp_0 = MultiLayerPerceptron(neurons_per_layer=layer_sizes, initializer=NormInitializer(), activation=Sigmoid())\n",
    "mlp_1 = MultiLayerPerceptron(neurons_per_layer=layer_sizes, initializer=NormInitializer(), activation=Sigmoid())\n",
    "mlp_0.initialize()\n",
    "mlp_1.initialize()\n",
    "\n",
    "for layer_id in range(len(layer_sizes) - 1):\n",
    "    mlp_1.layers[layer_id].weights = mlp_0.layers[layer_id].weights.copy()\n",
    "    mlp_1.layers[layer_id].bias = mlp_0.layers[layer_id].bias.copy()\n",
    "\n",
    "batch_size = 16\n",
    "x = np.random.uniform(-0.5, 0.5, size=(layer_sizes[0], batch_size))\n",
    "y = np.random.uniform(0, 1, size=(1, batch_size))\n",
    "\n",
    "probs = mlp_0.forward(x)\n",
    "grad = bce.backward(y, probs)\n",
    "mlp_0.backward(grad)\n",
    "\n",
    "for layer_id in range(len(layer_sizes) - 1):\n",
    "    print(\"Testing layer with id %s\" % layer_id)\n",
    "    if layer_id == 0:\n",
    "        activations_prev = x\n",
    "    else:\n",
    "        activations_prev = mlp_0.layers[layer_id - 1].activations\n",
    "    for i in range(layer_sizes[layer_id + 1]):\n",
    "        for j in range(layer_sizes[layer_id]):\n",
    "            mlp_1.layers[layer_id].weights[i, j] += epsilon\n",
    "            probs_plus = mlp_1.forward(x)\n",
    "\n",
    "            mlp_1.layers[layer_id].weights[i, j] -= 2 * epsilon\n",
    "            probs_minus = mlp_1.forward(x)\n",
    "\n",
    "            loss_plus = bce.forward(y, probs_plus)\n",
    "            loss_minus = bce.forward(y, probs_minus)\n",
    "\n",
    "            numeric_gradient = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            analytic_gradient = mlp_0.layers[layer_id].gradient_weights(activations_prev)[i, j] / batch_size\n",
    "            d = np.abs(numeric_gradient - analytic_gradient)\n",
    "\n",
    "            assert d <= tolerance, f\"Gradient check failed for layer {layer_id}, parameter index {i, j}: {d} > {tolerance}\"\n",
    "\n",
    "            mlp_1.layers[layer_id].weights[i, j] += epsilon\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f96baae-43a8-4238-a014-0988a90912f6",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4cbb8e6dc44fe2",
   "metadata": {},
   "source": [
    "Finally, lets prepare the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8401a88a84c67b7",
   "metadata": {},
   "source": [
    "Now we can train the model, first we have to define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "b195ea866f7e5566",
   "metadata": {},
   "source": [
    "x_shape = x_train[:, 0].shape[0]\n",
    "learning_rate = 5e-3\n",
    "batch_size = 16\n",
    "batches = int(np.ceil(x_train.shape[0] / batch_size))\n",
    "epochs = 100\n",
    "layer_sizes = [x_shape, 128, 128, 1]\n",
    "layer_sizes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e296bfd-a10d-4190-9c1c-302f8b4ed8d1",
   "metadata": {},
   "source": [
    "mlp = MultiLayerPerceptron(layer_sizes, ReLU(), NormInitializer())\n",
    "mlp.initialize()\n",
    "bce = BinaryCrossEntropy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "87a6d119a84d2f54",
   "metadata": {},
   "source": [
    "**Exercise 4: Implement the training step**"
   ]
  },
  {
   "cell_type": "code",
   "id": "7610dc69b55d00b5",
   "metadata": {},
   "source": [
    "def training_step(mlp: MultiLayerPerceptron, \n",
    "                  loss: BinaryCrossEntropy, \n",
    "                  x_batch: np.ndarray, \n",
    "                  y_batch: np.ndarray) -> float: \n",
    "    \"\"\"\n",
    "    Trains the network for one batch of data\n",
    "    :param mlp: the multi-layer perceptron instance\n",
    "    :param loss: the loss function instance\n",
    "    :param x_batch: the input data\n",
    "    :param y_batch: the labels\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### \n",
    "    ...\n",
    "    ### END YOUR CODE ### "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6a62b11c27c20cf9",
   "metadata": {},
   "source": [
    "**Click below for the solution**"
   ]
  },
  {
   "cell_type": "code",
   "id": "926c2ab6de59c29e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#@title Solution for the training step\n",
    "def training_step(mlp: MultiLayerPerceptron,\n",
    "                  loss: BinaryCrossEntropy, \n",
    "                  x_batch: np.ndarray, \n",
    "                  y_batch: np.ndarray) -> float: \n",
    "    \"\"\"\n",
    "    Trains the network for one batch of data\n",
    "    :param mlp: the multi-layer perceptron instance\n",
    "    :param loss: the loss function instance\n",
    "    :param x_batch: the input data\n",
    "    :param y_batch: the labels\n",
    "    :return: the loss for the batch\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### \n",
    "    probs = mlp.forward(x_batch)\n",
    "    grad_loss = loss.backward(y_batch, probs)\n",
    "    mlp.backward(grad_loss)\n",
    "    mlp.step_params(learning_rate)\n",
    "    return bce.forward(y, probs)\n",
    "    ### END YOUR CODE ###"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6d96299bcec95e1",
   "metadata": {},
   "source": [
    "x_train.shape, y_train.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "713a03ba0df93d57",
   "metadata": {},
   "source": [
    "Now we write out the training loop. We will store the training and validation losses for each epoch and plot them afterwards."
   ]
  },
  {
   "cell_type": "code",
   "id": "4e815786-5267-4bc7-bade-ba04f5e27da2",
   "metadata": {},
   "source": [
    "np.random.seed(42)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# loop over the epochs and the mini-batches, update the parameters and fill the values for the learning curves\n",
    "start = timer()\n",
    "mlp.initialize()\n",
    "\n",
    "progress = tqdm(range(epochs), desc=\"Training MLP\")\n",
    "for epoch in progress:\n",
    "\n",
    "    epoch_losses = []\n",
    "    for batch in range(batches):\n",
    "        batch_start, batch_end = batch * batch_size, (batch + 1) * batch_size\n",
    "        batch_end = min(batch_end, x_train.shape[1])\n",
    "        if batch_end - batch_start < 1:\n",
    "            continue\n",
    "        x = x_train[:, batch_start:batch_end]\n",
    "        y = y_train[:, batch_start:batch_end]\n",
    "    \n",
    "        loss = training_step(mlp, bce, x, y)\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "    train_losses.append(np.mean(epoch_losses))\n",
    "    pred_probs = mlp.forward(x_val)\n",
    "    val_loss = bce.forward(y_val, pred_probs)\n",
    "    val_losses.append(val_loss)\n",
    "    progress.set_postfix(epoch=epoch, train_loss=train_losses[-1], val_loss=val_losses[-1], refresh=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "308a6430fefe30d9",
   "metadata": {},
   "source": [
    "The validation curve"
   ]
  },
  {
   "cell_type": "code",
   "id": "222cbdf7b93c6c1b",
   "metadata": {},
   "source": [
    "plot_training_curve(train_losses, val_losses)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3198c6d7a667ec1f",
   "metadata": {},
   "source": [
    "def predict(mlp: MultiLayerPerceptron, inference_data: np.ndarray) -> np.ndarray:\n",
    "    pred: np.ndarray = mlp.forward(inference_data)\n",
    "    return (pred > 0.5).astype(int)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f632ee18289e9759",
   "metadata": {},
   "source": [
    "y_pred = predict(mlp, x_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f9b82947a664e513",
   "metadata": {},
   "source": [
    "Let's look at the performance on the test data, we can use scikit-learns built in utilities for this."
   ]
  },
  {
   "cell_type": "code",
   "id": "9e74e0454c3071f8",
   "metadata": {},
   "source": [
    "report = classification_report(\n",
    "    y_test.flatten(),\n",
    "    y_pred.flatten(),\n",
    "    target_names=list(labels.values()),\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {report['accuracy']}\")\n",
    "metrics = {k: v for k, v in report.items() if k not in ['accuracy']}\n",
    "df_report = pd.DataFrame(metrics).transpose()\n",
    "df_report = df_report[['precision', 'recall', 'f1-score', 'support']].round(4)\n",
    "df_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2eac5693891ba91f",
   "metadata": {},
   "source": "With the confusion matrix, we can see where the errors were made. In total, we only missclassified 8 samples on the test set, pretty good!"
  },
  {
   "cell_type": "code",
   "id": "403291993040159b",
   "metadata": {},
   "source": [
    "matrix = confusion_matrix(y_test.ravel(), y_pred.ravel())\n",
    "\n",
    "ConfusionMatrixDisplay(matrix, display_labels=list(labels.values())).plot(cmap=plt.cm.Blues)\n",
    "tick_marks = np.arange(len(list(labels.values())))\n",
    "plt.xticks(tick_marks, list(labels.values()))\n",
    "_ = plt.yticks(tick_marks, list(labels.values()))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
