{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5a1369",
   "metadata": {},
   "source": [
    "# Exercise: Working with APIs and Models\n",
    "\n",
    "This exercise introduces you to using state-of-the-art APIs/models (Segment Anything for Image segmentation). \n",
    "You will apply these tools to a medical imaging dataset (MedSegBench), practicing key concepts such as data splitting and working with pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9370a9",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "Ensure that you have the required libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f9cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python torch torchvision torchmetrics matplotlib pandas segment-anything medsegbench -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6378d5b9",
   "metadata": {},
   "source": [
    "### Download and Load the MedSegBench Dataset\n",
    "MedSegBench is a lightweight dataset tailored for medical segmentation tasks. \n",
    "Choose a subset (e.g., WbcMSBench) and load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12a1ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import medsegbench\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from medsegbench import INFO\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select an MedSegBench subset (e.g., WbcMSBench)\n",
    "data_flag = \"wbc\"\n",
    "info = INFO[data_flag]\n",
    "DataClass = getattr(medsegbench, info[\"python_class\"])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = DataClass(split=\"train\", download=True, size=256)\n",
    "test_dataset = DataClass(split=\"test\", download=True, size=256)\n",
    "\n",
    "# Create a full dataset by concatenating train + test; we will later do our own splits.\n",
    "all_images = np.concatenate([train_dataset.imgs, test_dataset.imgs], axis=0)\n",
    "all_labels = np.concatenate([train_dataset.labels, test_dataset.labels], axis=0)\n",
    "\n",
    "full_dataset = copy.deepcopy(train_dataset)\n",
    "full_dataset.imgs = all_images\n",
    "full_dataset.labels = all_labels\n",
    "\n",
    "print('Number of images', len(all_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc317646",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(train_dataset[0][0])\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(train_dataset[0][1])\n",
    "plt.title('Segmentation Mask')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "info['pixel_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4c851",
   "metadata": {},
   "source": [
    "## 2. Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b69485",
   "metadata": {},
   "source": [
    "Data splitting ensures that a machine learning model generalizes well on unseen data. \n",
    "The dataset is divided into training, validation, and test sets. \n",
    "Think of the training set as the material you use to prepare for an exam, the validation set as the practice tests to fine-tune your preparation, and the test set as the actual exam where your knowledge is assessed.\n",
    "\n",
    "When doing data splitting, always fix your random states, it makes your work reproducible. \n",
    "Also, when you execute the notebook multiple times, you always want the same image to be in the same set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fa3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified split\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    all_images,\n",
    "    all_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c86253",
   "metadata": {},
   "source": [
    "### Stratified Splitting\n",
    "\n",
    "Stratified splitting ensures that class proportions are maintained across splits, which is crucial for imbalanced datasets. \n",
    "Here's how you would implement it.\n",
    "However, this will be used purely illustratively since we're working with a segmentation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca96b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our random labels, this could indicate the collection centers, \n",
    "# or any other sample-level information\n",
    "rand_labels = np.random.randint(0, 2, size=len(all_images))\n",
    "\n",
    "# Stratified split\n",
    "dummy_train_data, dummy_test_data, dummy_train_labels, dummy_test_labels = train_test_split(\n",
    "    all_images,\n",
    "    all_labels,\n",
    "    test_size=0.2,\n",
    "    stratify=rand_labels,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Training data size: {len(dummy_train_data)}\")\n",
    "print(f\"Test data size: {len(dummy_test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4c386",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "Cross-validation divides data into k subsets, iteratively training on k-1 and validating on the remaining subset. \n",
    "This ensures the model is evaluated robustly across the dataset.\n",
    "In the lecture we used 10-fold cross-validation, thus k=10. Here we take k=5.\n",
    "Observe again that we fix the random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feacfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_index, (train_index, val_index) in enumerate(kf.split(train_dataset)):\n",
    "    train_data, val_data = train_dataset.imgs[train_index], train_dataset.imgs[val_index]\n",
    "    train_labels, val_labels = train_dataset.labels[train_index], train_dataset.labels[val_index]\n",
    "\n",
    "    print(f\"Fold: {fold_index}\")\n",
    "    print(f\"Training data size: {len(train_data)}\")\n",
    "    print(f\"Test data size: {len(val_data)}\\n\")\n",
    "\n",
    "    # Train and evaluate your model\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad08aa",
   "metadata": {},
   "source": [
    "## 3. Image Segmentation with SAM\n",
    "The Segment Anything Model (SAM) is designed to segment objects in images with minimal input. You will use it to generate masks for medical images.\n",
    "\n",
    "### Use the SAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c0af0",
   "metadata": {},
   "source": [
    "Let's use the first image of the test set as an example image to see SAM in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9785ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.asarray(test_dataset[0][0])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f64f7",
   "metadata": {},
   "source": [
    "### Selecting objects with SAM\n",
    "\n",
    "First, load the SAM model and predictor. \n",
    "Change the path below to point to the SAM checkpoint. \n",
    "Running on CUDA and using the default model are recommended for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a5e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"sam_vit_h_4b8939.pth\"):\n",
    "    !pip install wget\n",
    "    import wget\n",
    "    wget.download('https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth')\n",
    "    # If code does not work, try the following:\n",
    "    #!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cpu\"  # change to 'cuda' if available\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e256b12f",
   "metadata": {},
   "source": [
    "Process the image to produce an image embedding (latent vector or feature vector, which are outputs from the network or intermediate values within the network) by calling `SamPredictor.set_image`. \n",
    "`SamPredictor` remembers this embedding and will use it for subsequent mask prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad22537",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da802636",
   "metadata": {},
   "source": [
    "To select the nucleus of the cell of the stained white blood cell, choose a point on it. \n",
    "Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point).\n",
    "Multiple points can be input; here we use only one. \n",
    "The chosen point will be shown as a star on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e75a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[256//2, 256//2]])\n",
    "input_label = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deeccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis(\"on\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed79ca",
   "metadata": {},
   "source": [
    "Predict with `SamPredictor.predict`. The model returns masks, quality predictions for those masks, and low resolution mask logits that can be passed to the next iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d42b8",
   "metadata": {},
   "source": [
    "With `multimask_output=True` (the default setting), SAM outputs 3 masks, where `scores` gives the model's own estimation of the quality of these masks. This setting is intended for ambiguous input prompts, and helps the model disambiguate different objects consistent with the prompt. When `False`, it will return a single mask. For ambiguous prompts such as a single point, it is recommended to use `multimask_output=True` even if only a single mask is desired; the best single mask can be chosen by picking the one with the highest score returned in `scores`. This will often result in a better mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape  # (number_of_masks) x H x W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e148b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    show_mask(mask, plt.gca())\n",
    "    show_points(input_point, input_label, plt.gca())\n",
    "    plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817343b",
   "metadata": {},
   "source": [
    "### Specifying a specific object with additional points\n",
    "\n",
    "The single input point is ambiguous, and the model has returned multiple objects consistent with it. To obtain a single object, multiple points can be provided. If available, a mask from a previous iteration can also be supplied to the model to aid in prediction. When specifying a single object with multiple prompts, a single mask can be requested by setting `multimask_output=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[256//2, 256//2], [256//2+10, 256//2 + 10]])\n",
    "input_label = np.array([1, 1])\n",
    "\n",
    "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ce7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    mask_input=mask_input[None, :, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6953bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a581867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "show_mask(masks, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843ea98",
   "metadata": {},
   "source": [
    "To exclude the rest of the cell body (i.e., the cytoplasm) and specify just the nucleus of the cell, a background point (with label 0, here shown in red) can be supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_point = np.array([[80, 120], [160, 120]])\n",
    "input_label = np.array([1, 0])\n",
    "\n",
    "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    mask_input=mask_input[None, :, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "show_mask(masks, plt.gca())\n",
    "show_points(input_point, input_label, plt.gca())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87168d",
   "metadata": {},
   "source": [
    "### Specifying an object with a box\n",
    "\n",
    "The model can also take a box as input, provided in x1y1x2y2 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b579937",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_box = np.array([50, 30, 230, 230])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781d3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, _, _ = predictor.predict(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    box=input_box[None, :],\n",
    "    multimask_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "show_mask(masks[0], plt.gca())\n",
    "show_box(input_box, plt.gca())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f58e53",
   "metadata": {},
   "source": [
    "## 4. Evaluating Image Segmentation with SAM\n",
    "Now, we will use the training set of our dataset to choose the mask that can segment the cells nicely. Here, we have the assumption that they are usually located in the middle of the image.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bfe379",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the middle point of the image\n",
    "input_point = np.array([[256//2, 256//2]])\n",
    "input_label = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad4679",
   "metadata": {},
   "source": [
    "Let's look at whether our hypotheses about the middle alignment make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d42127",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 8, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(train_dataset[i][0])\n",
    "    show_points(input_point, input_label, ax, marker_size=200)\n",
    "    ax.axis(\"off\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcdc9c6",
   "metadata": {},
   "source": [
    "Our hypothesis looks good, but let's evaluate it on the test set.\n",
    "Since this takes a bit of time, we will limit ourselves to only doing it for the first 10 images, but you can run it for the full dataset if you have time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_masks = []\n",
    "pred_masks = []\n",
    "for i, data in tqdm(enumerate(test_dataset)):\n",
    "    # limit to the first 10 images\n",
    "    if i > 9:\n",
    "        break\n",
    "        \n",
    "    image, true_mask = data\n",
    "    \n",
    "    image = np.asarray(test_dataset[0][0])\n",
    "    predictor.set_image(image)\n",
    "    \n",
    "    pred_mask, _, _ = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        mask_input=mask_input[None, :, :],\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    \n",
    "    # we will only consider the task of segmenting the background from the cell, \n",
    "    # and thus discard the information if it's cytoplasm or a nucleus\n",
    "    true_mask[true_mask == 2] = 1\n",
    "    true_masks.append(true_mask)\n",
    "    pred_masks.append(pred_mask)\n",
    "    \n",
    "true_masks = np.stack(true_masks, axis=0)\n",
    "pred_masks = np.concatenate(pred_masks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ceba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Dice\n",
    "\n",
    "dice = Dice(average='micro')\n",
    "test_dice_score = dice(torch.Tensor(pred_masks).int(), torch.Tensor(true_masks).int()).cpu().numpy()\n",
    "\n",
    "print(f\"Test Dice Score: {test_dice_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa008aa",
   "metadata": {},
   "source": [
    "A dice score of $>0.85$ is quite good when we consider that we just selected the middle key point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95a303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
